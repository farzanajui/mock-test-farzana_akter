{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOUxzUkouCDlbIC8jeYuaF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farzanajui/mock-test-farzana_akter/blob/main/Mock_Exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EJIplDgm26B6"
      },
      "outputs": [],
      "source": [
        "# Mock Exam - Introduction to Business Programming\n",
        "# Course constraint: only use techniques taught Weeks 1–12\n",
        "# Include doctest examples and run doctest.testmod() after each section\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import doctest\n",
        "    doctest.testmod()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1 – Iterative Prompt Engineering Practice (30 marks)\n",
        "\n",
        "## 1.1 Initial Prompt & Pseudocode (8 marks)\n",
        "**My Prompt to AI:**\n",
        "> \"Generate pseudocode for a CLI Quiz Generator that loads Question|Answer pairs from a file using only Weeks 2 (lists), 6 (file I/O + validation), 7 (random) and 10 (planning).\"\n",
        "\n",
        "**AI Response (Pseudocode summary):**\n",
        "- start program  \n",
        "- read file with try/except; split by \"|\" and store (Q,A) tuples in a list  \n",
        "- ask user for N; validate it  \n",
        "- randomly select N questions  \n",
        "- loop through each Q/A, get answer, count score  \n",
        "- display score  \n",
        "\n",
        "**Course Reference:** Planning method from Week 10 – Algorithm Design.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1.2 Two Prompt Refinements (12 marks)\n",
        "\n",
        "**Refinement 1 Prompt:** Add case-insensitive checking + retry for invalid N (Weeks 6 & 7).  \n",
        "**AI Response (Summary):** Loop for valid input and use random.sample for unique selection.  \n",
        "\n",
        "**Refinement 2 Prompt:** Skip blank/malformed lines + show count (Week 6).  \n",
        "**AI Response (Summary):** Added try/except and count print statement.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1.3 Critical Analysis (10 marks)\n",
        "Write ≈150 words here explaining how your prompts improved and which Weeks guided you.\n"
      ],
      "metadata": {
        "id": "PHEhGm9b5S75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_chatbot():\n",
        "    \"\"\"\n",
        "    A basic chatbot that remembers the conversation (Week 6 level).\n",
        "\n",
        "    Manual run (interactive example):\n",
        "        Start the bot, type: hello <Enter>, quit <Enter>\n",
        "        Expect to see conversation history printed at the end.\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "ixRRBIx_EfX9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add(a, b):\n",
        "    \"\"\"\n",
        "    >>> add(2, 3)\n",
        "    5\n",
        "    \"\"\"\n",
        "    return a + b\n"
      ],
      "metadata": {
        "id": "T9Y1AEHFErdt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_chatbot():\n",
        "\n",
        "    memories = []\n",
        "    print(\"Chatbot started! Type 'quit' to exit.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        # Simple validation (Week 6)\n",
        "        if user_input.strip() == \"\":\n",
        "            print(\"Bot: Please type something.\")\n",
        "            continue\n",
        "\n",
        "        text = user_input.strip().lower()\n",
        "\n",
        "        # Always record the user's line first so history is complete\n",
        "        memories.append(f\"You: {user_input}\")\n",
        "\n",
        "        # Handle quit cleanly (no reply after quit)\n",
        "        if text == \"quit\":\n",
        "            print(\"Goodbye! Here's our conversation:\")\n",
        "            for m in memories:\n",
        "                print(m)\n",
        "            break\n",
        "\n",
        "        # Basic reply logic (Week 3 style if/elif)\n",
        "        if \"hello\" in text:\n",
        "            reply = \"Hello there!\"\n",
        "        elif \"how are you\" in text:\n",
        "            reply = \"I'm doing well, thanks!\"\n",
        "        elif \"weather\" in text:\n",
        "            reply = \"I don't know about weather, sorry!\"\n",
        "        else:\n",
        "            reply = \"That's interesting! Tell me more.\"\n",
        "\n",
        "        # Output + remember bot line\n",
        "        print(f\"Bot: {reply}\")\n",
        "        memories.append(f\"Bot: {reply}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import doctest\n",
        "    doctest.testmod()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "xXU4afvgFVdD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 3 – Debug & Refine Practice (Week 8 scope)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def refined_analyze_feedback(file_path):\n",
        "    \"\"\"\n",
        "    Analyze ratings from a CSV at a Week-8 complexity level.\n",
        "    Returns a dict on success, or None on any error.\n",
        "\n",
        "    >>> refined_analyze_feedback(\"\") is None\n",
        "    True\n",
        "    \"\"\"\n",
        "    # Week 6: simple input validation\n",
        "    if not file_path:\n",
        "        print(\"Error: No file path provided\")\n",
        "        return None\n",
        "\n",
        "    # Week 6: basic file reading with try/except\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except:\n",
        "        print(\"Error: Could not read file\")\n",
        "        return None\n",
        "\n",
        "    # Basic structural checks (Week 8)\n",
        "    if len(df) == 0:\n",
        "        print(\"Warning: File contains no data\")\n",
        "        return None\n",
        "    if 'rating' not in df.columns:\n",
        "        print(\"Error: 'rating' column not found\")\n",
        "        return None\n",
        "\n",
        "    # Week 8: numeric coercion + simple categorisation\n",
        "    try:\n",
        "        ratings = pd.to_numeric(df['rating'], errors='coerce').dropna()\n",
        "        if len(ratings) == 0:\n",
        "            print(\"Error: No valid numeric ratings\")\n",
        "            return None\n",
        "\n",
        "        avg = round(ratings.mean(), 2)\n",
        "        total = int(len(ratings))\n",
        "        pos = int((ratings >= 4).sum())\n",
        "        neg = int((ratings <= 2).sum())\n",
        "        neu = int(((ratings > 2) & (ratings < 4)).sum())\n",
        "\n",
        "        return {\n",
        "            \"average_rating\": avg,\n",
        "            \"total_responses\": total,\n",
        "            \"positive_count\": pos,\n",
        "            \"negative_count\": neg,\n",
        "            \"neutral_count\": neu,\n",
        "        }\n",
        "    except:\n",
        "        print(\"Error: Could not calculate ratings\")\n",
        "        return None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import doctest\n",
        "    doctest.testmod()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrU9t8AvahtW",
        "outputId": "0962737b-fa86-4c75-bb66-ebe946fd6935"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********************************************************************\n",
            "File \"__main__\", line 10, in __main__.refined_analyze_feedback\n",
            "Failed example:\n",
            "    refined_analyze_feedback(\"\") is None\n",
            "Expected:\n",
            "    True\n",
            "Got:\n",
            "    Error: No file path provided\n",
            "    True\n",
            "**********************************************************************\n",
            "1 items had failures:\n",
            "   1 of   1 in __main__.refined_analyze_feedback\n",
            "***Test Failed*** 1 failures.\n"
          ]
        }
      ]
    }
  ]
}